version: '3.8'

services:
  # NeuroSync Core API
  neurosync-api:
    build:
      context: .
      dockerfile: dockerfile
    ports:
      - "5000:5000"
    volumes:
      - .:/app
    environment:
      - USE_CUDA=${USE_CUDA:-auto}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - TTS_PROVIDER=${TTS_PROVIDER:-elevenlabs}
      - USE_LOCAL_LLM=${USE_LOCAL_LLM:-false}
    depends_on:
      - llama3-api
    networks:
      - neurosync-network

  # Llama 3.2 API Service
  llama3-api:
    build:
      context: ./utils/llm/local_api/llama3_2
      dockerfile: Dockerfile.llama3_2
    ports:
      - "5050:5050"
    volumes:
      - ./utils/llm/local_api/llama3_2:/app
    environment:
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - neurosync-network

networks:
  neurosync-network:
    driver: bridge 